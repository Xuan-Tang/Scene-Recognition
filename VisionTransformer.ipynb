{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VisionTransformer.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"14e7bb9f8d1548d38498a0f627cf93f4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7d7207d0c9fd4fcb86112f66f73880f0","IPY_MODEL_57b4268f0e9243b886d32220281850e8","IPY_MODEL_d6d7b4961b14481f8e2d4c3c083e6322"],"layout":"IPY_MODEL_26a9b399c5294ec19e88577bbb431f60"}},"7d7207d0c9fd4fcb86112f66f73880f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6238c3e349f04b07b8a04ac07f9a91d1","placeholder":"​","style":"IPY_MODEL_d8c2d21446dc4123ae1851241de67e67","value":"100%"}},"57b4268f0e9243b886d32220281850e8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4880dcfaf7d9452a8a0f94fba8d2ea01","max":531503671,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d9143e9ca120437e840328b5a8d876c5","value":531503671}},"d6d7b4961b14481f8e2d4c3c083e6322":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39b3bb44ce3742dfad27e1ba461a3e72","placeholder":"​","style":"IPY_MODEL_21cec8be45af44d1aa308ed9728b995d","value":" 507M/507M [00:04&lt;00:00, 105MB/s]"}},"26a9b399c5294ec19e88577bbb431f60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6238c3e349f04b07b8a04ac07f9a91d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8c2d21446dc4123ae1851241de67e67":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4880dcfaf7d9452a8a0f94fba8d2ea01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9143e9ca120437e840328b5a8d876c5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"39b3bb44ce3742dfad27e1ba461a3e72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21cec8be45af44d1aa308ed9728b995d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"WE9v-J-zQaIW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import Subset\n","from sklearn.model_selection import train_test_split\n","from torchvision.transforms import Compose, ToTensor, Resize\n","from torch.utils.data import DataLoader\n","from __future__ import print_function, division\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import torch.backends.cudnn as cudnn\n","import numpy as np\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","\n","cudnn.benchmark = True\n","plt.ion()   # interactive mode"],"metadata":{"id":"bR0Xg9L8ZWmZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title\n","import os\n","from os import path\n","from typing import Any, Callable, Dict, List, Optional, Tuple\n","from urllib.parse import urljoin\n","\n","from torchvision.datasets.folder import default_loader\n","from torchvision.datasets.utils import verify_str_arg, check_integrity, download_and_extract_archive\n","from torchvision.datasets.vision import VisionDataset\n","\n","#modify the original Places365 loader to load only data starting with a\n","class Places365c(VisionDataset):\n","    r\"\"\"`Places365 <http://places2.csail.mit.edu/index.html>`_ classification dataset.\n","\n","    Args:\n","        root (string): Root directory of the Places365 dataset.\n","        split (string, optional): The dataset split. Can be one of ``train-standard`` (default), ``train-challenge``,\n","            ``val``.\n","        small (bool, optional): If ``True``, uses the small images, i. e. resized to 256 x 256 pixels, instead of the\n","            high resolution ones.\n","        download (bool, optional): If ``True``, downloads the dataset components and places them in ``root``. Already\n","            downloaded archives are not downloaded again.\n","        transform (callable, optional): A function/transform that  takes in an PIL image\n","            and returns a transformed version. E.g, ``transforms.RandomCrop``\n","        target_transform (callable, optional): A function/transform that takes in the\n","            target and transforms it.\n","        loader (callable, optional): A function to load an image given its path.\n","\n","     Attributes:\n","        classes (list): List of the class names.\n","        class_to_idx (dict): Dict with items (class_name, class_index).\n","        imgs (list): List of (image path, class_index) tuples\n","        targets (list): The class_index value for each image in the dataset\n","\n","    Raises:\n","        RuntimeError: If ``download is False`` and the meta files, i. e. the devkit, are not present or corrupted.\n","        RuntimeError: If ``download is True`` and the image archive is already extracted.\n","    \"\"\"\n","    _SPLITS = (\"train-standard\", \"train-challenge\", \"val\")\n","    _BASE_URL = \"http://data.csail.mit.edu/places/places365/\"\n","    # {variant: (archive, md5)}\n","    _DEVKIT_META = {\n","        \"standard\": (\"filelist_places365-standard.tar\", \"35a0585fee1fa656440f3ab298f8479c\"),\n","        \"challenge\": (\"filelist_places365-challenge.tar\", \"70a8307e459c3de41690a7c76c931734\"),\n","    }\n","    # (file, md5)\n","    _CATEGORIES_META = (\"categories_places365.txt\", \"06c963b85866bd0649f97cb43dd16673\")\n","    # {split: (file, md5)}\n","    _FILE_LIST_META = {\n","        \"train-standard\": (\"places365_train_standard.txt\", \"a026646418c935c845bb4f294e6b8497\"),\n","        \"train-challenge\": (\"places365_train_challenge.txt\", \"b2931dc997b8c33c27e7329c073a6b57\"),\n","        \"val\": (\"places365_val.txt\", \"e9f2fd57bfd9d07630173f4e8708e4b1\"),\n","    }\n","    # {(split, small): (file, md5)}\n","    _IMAGES_META = {\n","        (\"train-standard\", False): (\"train_large_places365standard.tar\", \"67e186b496a84c929568076ed01a8aa1\"),\n","        (\"train-challenge\", False): (\"train_large_places365challenge.tar\", \"605f18e68e510c82b958664ea134545f\"),\n","        (\"val\", False): (\"val_large.tar\", \"9b71c4993ad89d2d8bcbdc4aef38042f\"),\n","        (\"train-standard\", True): (\"train_256_places365standard.tar\", \"53ca1c756c3d1e7809517cc47c5561c5\"),\n","        (\"train-challenge\", True): (\"train_256_places365challenge.tar\", \"741915038a5e3471ec7332404dfb64ef\"),\n","        (\"val\", True): (\"val_256.tar\", \"e27b17d8d44f4af9a78502beb927f808\"),\n","    }\n","\n","    def __init__(\n","        self,\n","        root: str,\n","        split: str = \"train-standard\",\n","        small: bool = False,\n","        download: bool = False,\n","        transform: Optional[Callable] = None,\n","        target_transform: Optional[Callable] = None,\n","        loader: Callable[[str], Any] = default_loader,\n","    ) -> None:\n","        super().__init__(root, transform=transform, target_transform=target_transform)\n","\n","        self.split = self._verify_split(split)\n","        self.small = small\n","        self.loader = loader\n","\n","        self.classes, self.class_to_idx = self.load_categories(download)\n","        self.imgs, self.targets = self.load_file_list(download)\n","\n","        if download:\n","            self.download_images()\n","\n","    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n","        file, target = self.imgs[index]\n","        image = self.loader(file)\n","\n","        if self.transforms is not None:\n","            image, target = self.transforms(image, target)\n","\n","        return image, target\n","\n","\n","    def __len__(self) -> int:\n","        return len(self.imgs)\n","\n","    @property\n","    def variant(self) -> str:\n","        return \"challenge\" if \"challenge\" in self.split else \"standard\"\n","\n","    @property\n","    def images_dir(self) -> str:\n","        size = \"256\" if self.small else \"large\"\n","        if self.split.startswith(\"train\"):\n","            dir = f\"data_{size}_{self.variant}\"\n","        else:\n","            dir = f\"{self.split}_{size}\"\n","        return path.join(self.root, dir)\n","\n","    def load_categories(self, download: bool = True) -> Tuple[List[str], Dict[str, int]]:\n","        def process(line: str) -> Tuple[str, int]:\n","            cls, idx = line.split()\n","            return cls, int(idx)\n","\n","        file, md5 = self._CATEGORIES_META\n","        file = path.join(self.root, file)\n","\n","        with open(file) as fh:\n","            class_to_idx = dict(process(line) for line in fh)\n","\n","        return sorted(class_to_idx.keys()), class_to_idx\n","\n","    def load_file_list(self, download: bool = True) -> Tuple[List[Tuple[str, int]], List[int]]:\n","        def process(line: str, sep=\"/\") -> Tuple[str, int]:\n","            image, idx = line.split()\n","            return path.join(self.images_dir, image.lstrip(sep).replace(sep, os.sep)), int(idx)\n","\n","        file, md5 = self._FILE_LIST_META[self.split]\n","        file = path.join(self.root, file)\n","        if not self._check_integrity(file, md5, download):\n","            self.download_devkit()\n","\n","        with open(file) as fh:\n","            images = [process(line) for line in fh]\n","\n","        _, targets = zip(*images)\n","        return images, list(targets)\n","\n","    def download_devkit(self) -> None:\n","        file, md5 = self._DEVKIT_META[self.variant]\n","        download_and_extract_archive(urljoin(self._BASE_URL, file), self.root, md5=md5)\n","\n","    def download_images(self) -> None:\n","        if path.exists(self.images_dir):\n","            raise RuntimeError(\n","                f\"The directory {self.images_dir} already exists. If you want to re-download or re-extract the images, \"\n","                f\"delete the directory.\"\n","            )\n","\n","        file, md5 = self._IMAGES_META[(self.split, self.small)]\n","        download_and_extract_archive(urljoin(self._BASE_URL, file), self.root, md5=md5)\n","\n","        if self.split.startswith(\"train\"):\n","            os.rename(self.images_dir.rsplit(\"_\", 1)[0], self.images_dir)\n","\n","    def extra_repr(self) -> str:\n","        return \"\\n\".join((\"Split: {split}\", \"Small: {small}\")).format(**self.__dict__)\n","\n","    def _verify_split(self, split: str) -> str:\n","        return verify_str_arg(split, \"split\", self._SPLITS)\n","\n","    def _check_integrity(self, file: str, md5: str, download: bool) -> bool:\n","        integrity = check_integrity(file, md5=md5)\n","        if not integrity and not download:\n","            raise RuntimeError(\n","                f\"The file {file} does not exist or is corrupted. You can set download=True to download it.\"\n","            )\n","        return integrity"],"metadata":{"id":"GKtls523G_5w","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/My\\ Drive/\n","!pwd"],"metadata":{"id":"9c2xM_onhqVw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650246659894,"user_tz":240,"elapsed":1021,"user":{"displayName":"Fanyue Xia","userId":"09346053328437952788"}},"outputId":"3dba242a-ef04-494f-e5e2-8a80d9975a3b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive\n","/content/drive/My Drive\n"]}]},{"cell_type":"code","source":["#@title\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(224),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ])\n","}"],"metadata":{"id":"ITJ2fvlYaJAW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title\n","from torchvision.transforms.functional import to_tensor\n","train_dataset=Places365c(root=\"Places365\", small=True, download=False, transform=data_transforms[\"train\"])\n","other_dataset=Places365c(root=\"Places365\", small=True, download=False, transform=data_transforms[\"val\"])"],"metadata":{"id":"Ae8KtpMPbOvz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title\n","def train_val_test_dataset(train_dataset,other_dataset):\n","    train_idx, test_idx= train_test_split(list(range(len(other_dataset))), test_size=0.2,shuffle=True)\n","    datasets = {}\n","    datasets['test'] = Subset(other_dataset, test_idx)\n","    tmp_val = Subset(other_dataset, train_idx)\n","    tmp_train = Subset(train_dataset, train_idx)\n","    train_idx, val_idx = train_test_split(list(range(len(tmp_val))), test_size=0.1)\n","    datasets['train'] = Subset(tmp_train, train_idx)\n","    datasets['val'] = Subset(tmp_val, val_idx)\n","    return datasets"],"metadata":{"id":"FAX2ifb_Qm7T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title\n","def train_test_dataset(train_dataset,other_dataset):\n","    train_idx, test_idx= train_test_split(list(range(len(other_dataset))), test_size=0.2,shuffle=True)\n","    datasets = {}\n","    datasets['val'] = Subset(other_dataset, test_idx)\n","    datasets['train'] = Subset(train_dataset, test_idx)\n","    return datasets"],"metadata":{"id":"IS7mj8Mjjoht","executionInfo":{"status":"ok","timestamp":1650251290447,"user_tz":240,"elapsed":326,"user":{"displayName":"Fanyue Xia","userId":"09346053328437952788"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["#@title\n","class_names = [x[3:] for x in train_dataset.load_categories(False)[0]]\n","num_classes = len(class_names)\n","batch_size = 8\n","num_epochs = 10\n","feature_extract = True\n","model_name = \"vgg\"\n"],"metadata":{"id":"8AdXo-GIQKTf","executionInfo":{"status":"aborted","timestamp":1650249764486,"user_tz":240,"elapsed":3,"user":{"displayName":"Fanyue Xia","userId":"09346053328437952788"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_datasets = train_test_dataset(train_dataset,other_dataset)\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n","dataloaders_dict = {x:DataLoader(image_datasets[x],batch_size=batch_size, shuffle=True, num_workers=10) for x in ['train','val']}\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"GP32M3zqKAUR","executionInfo":{"status":"ok","timestamp":1650251292440,"user_tz":240,"elapsed":4,"user":{"displayName":"Fanyue Xia","userId":"09346053328437952788"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"78d135be-084c-47e2-ab2a-cbb727da9813"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"code","source":["print(class_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zcf_AONEQgKD","executionInfo":{"status":"ok","timestamp":1650248651145,"user_tz":240,"elapsed":353,"user":{"displayName":"Fanyue Xia","userId":"09346053328437952788"}},"outputId":"eee1b04e-eb85-4a6a-9b72-c4060481ed4c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['airfield', 'airplane_cabin', 'airport_terminal', 'alcove', 'alley', 'amphitheater', 'amusement_arcade', 'amusement_park', 'apartment_building/outdoor', 'aquarium', 'aqueduct', 'arcade', 'arch', 'archaelogical_excavation', 'archive', 'arena/hockey', 'arena/performance', 'arena/rodeo', 'army_base', 'art_gallery', 'art_school', 'art_studio', 'artists_loft', 'assembly_line', 'athletic_field/outdoor', 'atrium/public', 'attic', 'auditorium', 'auto_factory', 'auto_showroom']\n"]}]},{"cell_type":"code","source":["def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n","    since = time.time()\n","\n","    val_acc_history = []\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    # Get model outputs and calculate loss\n","                    # Special case for inception because in training it has an auxiliary output. In train\n","                    #   mode we calculate the loss by summing the final output and the auxiliary output\n","                    #   but in testing we only consider the final output.\n","                    if is_inception and phase == 'train':\n","                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n","                        outputs, aux_outputs = model(inputs)\n","                        loss1 = criterion(outputs, labels)\n","                        loss2 = criterion(aux_outputs, labels)\n","                        loss = loss1 + 0.4*loss2\n","                    else:\n","                        outputs = model(inputs)\n","                        loss = criterion(outputs, labels)\n","\n","                    _, preds = torch.max(outputs, 1)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","            if phase == 'val':\n","                val_acc_history.append(epoch_acc)\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model, val_acc_history"],"metadata":{"id":"SEpvbqB0fFyA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def set_parameter_requires_grad(model, feature_extracting):\n","    if feature_extracting:\n","        for param in model.parameters():\n","            param.requires_grad = False"],"metadata":{"id":"6BaB_KGcfHF2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n","    # Initialize these variables which will be set in this if statement. Each of these\n","    #   variables is model specific.\n","    model_ft = None\n","    input_size = 0\n","\n","    if model_name == \"resnet\":\n","        \"\"\" Resnet18\n","        \"\"\"\n","        model_ft = models.resnet18(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"alexnet\":\n","        \"\"\" Alexnet\n","        \"\"\"\n","        model_ft = models.alexnet(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier[6].in_features\n","        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"vgg\":\n","        \"\"\" VGG11_bn\n","        \"\"\"\n","        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier[6].in_features\n","        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"squeezenet\":\n","        \"\"\" Squeezenet\n","        \"\"\"\n","        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n","        model_ft.num_classes = num_classes\n","        input_size = 224\n","\n","    elif model_name == \"densenet\":\n","        \"\"\" Densenet\n","        \"\"\"\n","        model_ft = models.densenet121(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier.in_features\n","        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"inception\":\n","        \"\"\" Inception v3\n","        Be careful, expects (299,299) sized images and has auxiliary output\n","        \"\"\"\n","        model_ft = models.inception_v3(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        # Handle the auxilary net\n","        num_ftrs = model_ft.AuxLogits.fc.in_features\n","        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n","        # Handle the primary net\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n","        input_size = 299\n","\n","    else:\n","        print(\"Invalid model name, exiting...\")\n","        exit()\n","\n","    return model_ft, input_size\n","\n","# Initialize the model for this run\n","model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n","\n","# Print the model we just instantiated\n","print(model_ft)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":835,"referenced_widgets":["14e7bb9f8d1548d38498a0f627cf93f4","7d7207d0c9fd4fcb86112f66f73880f0","57b4268f0e9243b886d32220281850e8","d6d7b4961b14481f8e2d4c3c083e6322","26a9b399c5294ec19e88577bbb431f60","6238c3e349f04b07b8a04ac07f9a91d1","d8c2d21446dc4123ae1851241de67e67","4880dcfaf7d9452a8a0f94fba8d2ea01","d9143e9ca120437e840328b5a8d876c5","39b3bb44ce3742dfad27e1ba461a3e72","21cec8be45af44d1aa308ed9728b995d"]},"id":"yCuNg2ubfJyD","executionInfo":{"status":"ok","timestamp":1650248770353,"user_tz":240,"elapsed":6666,"user":{"displayName":"Fanyue Xia","userId":"09346053328437952788"}},"outputId":"cb06b19c-8363-40d8-c777-a940e40ea0e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/vgg11_bn-6002323d.pth\" to /root/.cache/torch/hub/checkpoints/vgg11_bn-6002323d.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/507M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14e7bb9f8d1548d38498a0f627cf93f4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["VGG(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (6): ReLU(inplace=True)\n","    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): ReLU(inplace=True)\n","    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (13): ReLU(inplace=True)\n","    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (17): ReLU(inplace=True)\n","    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (20): ReLU(inplace=True)\n","    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (24): ReLU(inplace=True)\n","    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (27): ReLU(inplace=True)\n","    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","  (classifier): Sequential(\n","    (0): Linear(in_features=25088, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=4096, out_features=30, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"code","source":["# Send the model to GPU\n","model_ft = model_ft.to(device)\n","\n","# Gather the parameters to be optimized/updated in this run. If we are\n","#  finetuning we will be updating all parameters. However, if we are\n","#  doing feature extract method, we will only update the parameters\n","#  that we have just initialized, i.e. the parameters with requires_grad\n","#  is True.\n","params_to_update = model_ft.parameters()\n","print(\"Params to learn:\")\n","if feature_extract:\n","    params_to_update = []\n","    for name,param in model_ft.named_parameters():\n","        if param.requires_grad == True:\n","            params_to_update.append(param)\n","            print(\"\\t\",name)\n","else:\n","    for name,param in model_ft.named_parameters():\n","        if param.requires_grad == True:\n","            print(\"\\t\",name)\n","\n","# Observe that all parameters are being optimized\n","optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kBy5XFlVf4gc","executionInfo":{"status":"ok","timestamp":1650248939241,"user_tz":240,"elapsed":409,"user":{"displayName":"Fanyue Xia","userId":"09346053328437952788"}},"outputId":"91aca9aa-87e4-443e-dc59-5be89b6da516"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Params to learn:\n","\t classifier.6.weight\n","\t classifier.6.bias\n"]}]},{"cell_type":"code","source":["# Setup the loss fxn\n","criterion = nn.CrossEntropyLoss()\n","\n","# Train and evaluate\n","model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KdSA8WCRf6_6","outputId":"23cec632-cdea-4152-df79-0ab85acab8bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0/14\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 1.4493 Acc: 0.5650\n","val Loss: 0.9160 Acc: 0.7081\n","\n","Epoch 1/14\n","----------\n","train Loss: 1.4253 Acc: 0.5709\n","val Loss: 0.8696 Acc: 0.7256\n","\n","Epoch 2/14\n","----------\n","train Loss: 1.4188 Acc: 0.5731\n","val Loss: 0.8468 Acc: 0.7287\n","\n","Epoch 3/14\n","----------\n","train Loss: 1.4221 Acc: 0.5739\n","val Loss: 0.8290 Acc: 0.7374\n","\n","Epoch 4/14\n","----------\n","train Loss: 1.4168 Acc: 0.5771\n","val Loss: 0.8108 Acc: 0.7411\n","\n","Epoch 5/14\n","----------\n","train Loss: 1.4193 Acc: 0.5803\n","val Loss: 0.8158 Acc: 0.7397\n","\n","Epoch 6/14\n","----------\n","train Loss: 1.4197 Acc: 0.5763\n","val Loss: 0.7840 Acc: 0.7463\n","\n","Epoch 7/14\n","----------\n","train Loss: 1.4009 Acc: 0.5798\n","val Loss: 0.7974 Acc: 0.7434\n","\n","Epoch 8/14\n","----------\n","train Loss: 1.4257 Acc: 0.5783\n","val Loss: 0.8087 Acc: 0.7404\n","\n","Epoch 9/14\n","----------\n","train Loss: 1.4167 Acc: 0.5783\n","val Loss: 0.7707 Acc: 0.7474\n","\n","Epoch 10/14\n","----------\n"]}]}]}